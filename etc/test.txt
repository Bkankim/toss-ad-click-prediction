# CTR ë°ì´í„° í•µì‹¬ íŒ¨í„´ ë¶„ì„
# ë‹¤ìŒ ë‹¨ê³„: ì‹œê°„ íŒ¨í„´, ë¶ˆê· í˜• ì²˜ë¦¬, ê²°ì¸¡ê°’ ì „ëµ

import cudf
import cupy as cp
import pandas as pd
import numpy as np
import pyarrow.parquet as pq
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import gc

print("ğŸ¯ CTR í•µì‹¬ íŒ¨í„´ ë¶„ì„")
print("=" * 60)

def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    try:
        mempool = cp.get_default_memory_pool()
        mempool.free_all_blocks()
        gc.collect()
    except:
        gc.collect()

: {e}")
        return None

# 1. ì‹œê°„ íŒ¨í„´ ì‹¬ì¸µ ë¶„ì„
def analyze_temporal_patterns(gdf):
    """ì‹œê°„ ê´€ë ¨ íŒ¨í„´ ë¶„ì„"""
    print(f"\nğŸ“… ì‹œê°„ íŒ¨í„´ ë¶„ì„")
    
    # ì‹œê°„ ì»¬ëŸ¼ í™•ì¸
    time_columns = ['day_of_week', 'hour', 'seq']
    available_time_cols = [col for col in time_columns if col in gdf.columns]
    
    print(f"   ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼: {available_time_cols}")
    
    temporal_insights = {}
    
    # 1.1 ìš”ì¼ë³„ íŒ¨í„´
    if 'day_of_week' in available_time_cols:
        print(f"\n   ğŸ“Š ìš”ì¼ë³„ í´ë¦­ íŒ¨í„´:")
        dow_stats = gdf.groupby('day_of_week').agg({
            'clicked': ['count', 'sum', 'mean']
        }).to_pandas()
        
        dow_stats.columns = ['ì´_ë…¸ì¶œ', 'ì´_í´ë¦­', 'í´ë¦­ë¥ ']
        print(dow_stats)
        
        # ì£¼ë§/í‰ì¼ ë¹„êµ
        weekend_data = gdf[gdf['day_of_week'].isin([5, 6])]
        weekday_data = gdf[~gdf['day_of_week'].isin([5, 6])]
        
        weekend_ctr = weekend_data['clicked'].mean()
        weekday_ctr = weekday_data['clicked'].mean()
        
        print(f"   ğŸ¯ ì£¼ë§ í´ë¦­ë¥ : {weekend_ctr:.4f}")
        print(f"   ğŸ¯ í‰ì¼ í´ë¦­ë¥ : {weekday_ctr:.4f}")
        print(f"   ğŸ¯ ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ctr/weekday_ctr:.2f}")
        
        temporal_insights['weekend_boost'] = weekend_ctr / weekday_ctr
    
    # 1.2 ì‹œê°„ëŒ€ë³„ íŒ¨í„´
    if 'hour' in available_time_cols:
        print(f"\n   ğŸ• ì‹œê°„ëŒ€ë³„ í´ë¦­ íŒ¨í„´:")
        hour_stats = gdf.groupby('hour').agg({
            'clicked': ['count', 'sum', 'mean']
        }).to_pandas()
        
        hour_stats.columns = ['ì´_ë…¸ì¶œ', 'ì´_í´ë¦­', 'í´ë¦­ë¥ ']
        
        # í”¼í¬ ì‹œê°„ëŒ€ ì°¾ê¸°
        peak_hours = hour_stats.nlargest(3, 'í´ë¦­ë¥ ').index.tolist()
        low_hours = hour_stats.nsmallest(3, 'í´ë¦­ë¥ ').index.tolist()
        
        print(f"   ğŸ”¥ ê³  í´ë¦­ ì‹œê°„ëŒ€: {peak_hours}")
        print(f"   ğŸ’¤ ì € í´ë¦­ ì‹œê°„ëŒ€: {low_hours}")
        
        # ì‹œê°„ëŒ€ ê·¸ë£¹ë³„ ì„±ê³¼
        morning = hour_stats[hour_stats.index.isin([6,7,8,9,10,11])]['í´ë¦­ë¥ '].mean()
        afternoon = hour_stats[hour_stats.index.isin([12,13,14,15,16,17])]['í´ë¦­ë¥ '].mean()
        evening = hour_stats[hour_stats.index.isin([18,19,20,21,22])]['í´ë¦­ë¥ '].mean()
        night = hour_stats[hour_stats.index.isin([23,0,1,2,3,4,5])]['í´ë¦­ë¥ '].mean()
        
        print(f"   ğŸŒ… ì˜¤ì „(6-11): {morning:.4f}")
        print(f"   â˜€ï¸ ì˜¤í›„(12-17): {afternoon:.4f}")
        print(f"   ğŸŒ† ì €ë…(18-22): {evening:.4f}")
        print(f"   ğŸŒ™ ë°¤(23-5): {night:.4f}")
        
        temporal_insights['peak_hours'] = peak_hours
        temporal_insights['best_period'] = max([
            ('ì˜¤ì „', morning), ('ì˜¤í›„', afternoon), 
            ('ì €ë…', evening), ('ë°¤', night)
        ], key=lambda x: x[1])
    
    # 1.3 seq ì»¬ëŸ¼ ë¶„ì„ (ìˆœì„œ/ì‹œê°„ ì •ë³´)
    if 'seq' in available_time_cols:
        print(f"\n   ğŸ“ˆ ì‹œí€€ìŠ¤(seq) íŒ¨í„´ ë¶„ì„:")
        
        # seqì™€ í´ë¦­ë¥ ì˜ ê´€ê³„
        seq_bins = cp.linspace(gdf['seq'].min(), gdf['seq'].max(), 20)
        gdf['seq_bin'] = cp.digitize(gdf['seq'], seq_bins)
        
        seq_stats = gdf.groupby('seq_bin').agg({
            'clicked': ['count', 'mean']
        }).to_pandas()
        seq_stats.columns = ['ë…¸ì¶œìˆ˜', 'í´ë¦­ë¥ ']
        
        print("   ì‹œí€€ìŠ¤ êµ¬ê°„ë³„ í´ë¦­ë¥  (ìƒìœ„ 5ê°œ):")
        print(seq_stats.nlargest(5, 'í´ë¦­ë¥ '))
        
        # seq ë²”ìœ„ ë¶„ì„
        seq_min, seq_max = float(gdf['seq'].min()), float(gdf['seq'].max())
        seq_range = seq_max - seq_min
        
        print(f"   ğŸ¯ seq ë²”ìœ„: {seq_min:.0f} ~ {seq_max:.0f} (ë²”ìœ„: {seq_range:.0f})")
        
        temporal_insights['seq_range'] = (seq_min, seq_max)
    
    return temporal_insights

# 2. ê²°ì¸¡ê°’ ì‹¬ì¸µ ë¶„ì„
def analyze_missing_patterns(gdf):
    """ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ë° ì „ëµ ìˆ˜ë¦½"""
    print(f"\nâ“ ê²°ì¸¡ê°’ ì‹¬ì¸µ ë¶„ì„")
    
    # ê²°ì¸¡ê°’ ë¹„ìœ¨ ê³„ì‚°
    missing_stats = []
    for col in gdf.columns:
        if col != 'clicked':
            missing_count = gdf[col].isnull().sum()
            missing_pct = (missing_count / len(gdf)) * 100
            
            if missing_count > 0:
                missing_stats.append({
                    'column': col,
                    'missing_count': missing_count,
                    'missing_pct': missing_pct,
                    'dtype': str(gdf[col].dtype)
                })
    
    # ê²°ì¸¡ê°’ ì‹¬ê°ë„ë³„ ë¶„ë¥˜
    missing_df = pd.DataFrame(missing_stats).sort_values('missing_pct', ascending=False)
    
    severe_missing = missing_df[missing_df['missing_pct'] > 50]  # 50% ì´ìƒ
    moderate_missing = missing_df[(missing_df['missing_pct'] > 10) & (missing_df['missing_pct'] <= 50)]  # 10-50%
    light_missing = missing_df[missing_df['missing_pct'] <= 10]  # 10% ì´í•˜
    
    print(f"   ğŸš¨ ì‹¬ê°í•œ ê²°ì¸¡ (>50%): {len(severe_missing)}ê°œ")
    print(f"   âš ï¸ ë³´í†µ ê²°ì¸¡ (10-50%): {len(moderate_missing)}ê°œ")
    print(f"   ğŸ’¡ ê°€ë²¼ìš´ ê²°ì¸¡ (<10%): {len(light_missing)}ê°œ")
    
    # ìƒìœ„ 10ê°œ ê²°ì¸¡ ì»¬ëŸ¼
    print(f"\n   ğŸ“Š ê²°ì¸¡ê°’ ìƒìœ„ 10ê°œ ì»¬ëŸ¼:")
    for _, row in missing_df.head(10).iterrows():
        print(f"      {row['column']}: {row['missing_pct']:.1f}% ({row['dtype']})")
    
    # ê²°ì¸¡ íŒ¨í„´ê³¼ í´ë¦­ë¥ ì˜ ê´€ê³„
    print(f"\n   ğŸ¯ ê²°ì¸¡ê°’ê³¼ í´ë¦­ë¥  ê´€ê³„:")
    
    # ëª‡ ê°œ ì£¼ìš” ê²°ì¸¡ ì»¬ëŸ¼ì— ëŒ€í•´ ë¶„ì„
    for _, row in missing_df.head(5).iterrows():
        col = row['column']
        
        # ê²°ì¸¡ ì—¬ë¶€ì— ë”°ë¥¸ í´ë¦­ë¥  ë¹„êµ
        missing_mask = gdf[col].isnull()
        
        ctr_with_missing = gdf[missing_mask]['clicked'].mean() if missing_mask.sum() > 0 else 0
        ctr_without_missing = gdf[~missing_mask]['clicked'].mean() if (~missing_mask).sum() > 0 else 0
        
        if ctr_with_missing > 0 and ctr_without_missing > 0:
            ratio = ctr_with_missing / ctr_without_missing
            print(f"      {col}: ê²°ì¸¡ì‹œ {ctr_with_missing:.4f} vs ì •ìƒì‹œ {ctr_without_missing:.4f} (ë¹„ìœ¨: {ratio:.2f})")
    
    return {
        'severe_missing': severe_missing,
        'moderate_missing': moderate_missing,
        'light_missing': light_missing,
        'total_missing_cols': len(missing_df)
    }

# 3. íŠ¹ì„±ë³„ í´ë¦­ë¥  ë¶„ì„
def analyze_feature_impact(gdf):
    """íŠ¹ì„±ë³„ í´ë¦­ë¥  ì˜í–¥ë„ ë¶„ì„"""
    print(f"\nğŸ¯ íŠ¹ì„±ë³„ í´ë¦­ë¥  ì˜í–¥ë„ ë¶„ì„")
    
    feature_impacts = {}
    
    # ë²”ì£¼í˜• íŠ¹ì„± ë¶„ì„
    categorical_cols = gdf.select_dtypes(include=['object']).columns
    print(f"   ğŸ“Š ë²”ì£¼í˜• íŠ¹ì„± ({len(categorical_cols)}ê°œ):")
    
    for col in categorical_cols:
        if col in gdf.columns and col != 'clicked':
            try:
                # ì¹´í…Œê³ ë¦¬ë³„ í´ë¦­ë¥ 
                cat_stats = gdf.groupby(col).agg({
                    'clicked': ['count', 'sum', 'mean']
                }).to_pandas()
                
                cat_stats.columns = ['ë…¸ì¶œìˆ˜', 'í´ë¦­ìˆ˜', 'í´ë¦­ë¥ ']
                
                # í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸
                click_rates = cat_stats['í´ë¦­ë¥ '].values
                ctr_std = np.std(click_rates)
                ctr_range = np.max(click_rates) - np.min(click_rates)
                
                print(f"      {col}: CTR ë²”ìœ„ {np.min(click_rates):.4f}~{np.max(click_rates):.4f}, "
                      f"í‘œì¤€í¸ì°¨ {ctr_std:.4f}")
                
                feature_impacts[col] = {
                    'type': 'categorical',
                    'ctr_range': ctr_range,
                    'ctr_std': ctr_std,
                    'categories': len(cat_stats)
                }
                
            except Exception as e:
                print(f"      {col}: ë¶„ì„ ì‹¤íŒ¨ - {e}")
    
    # ìˆ˜ì¹˜í˜• íŠ¹ì„± ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒìœ„ 10ê°œë§Œ)
    numeric_cols = gdf.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns
    numeric_cols = [col for col in numeric_cols if col != 'clicked'][:10]
    
    print(f"\n   ğŸ“ˆ ìˆ˜ì¹˜í˜• íŠ¹ì„± ìƒê´€ê´€ê³„ (ìƒìœ„ 10ê°œ):")
    
    for col in numeric_cols:
        try:
            # clickedì™€ì˜ ìƒê´€ê´€ê³„
            correlation = float(gdf[[col, 'clicked']].corr().iloc[0, 1])
            
            # ë¶„ìœ„ìˆ˜ë³„ í´ë¦­ë¥ 
            quartiles = gdf[col].quantile([0.25, 0.5, 0.75]).to_pandas()
            
            q1_ctr = gdf[gdf[col] <= quartiles[0.25]]['clicked'].mean()
            q4_ctr = gdf[gdf[col] >= quartiles[0.75]]['clicked'].mean()
            
            ctr_diff = abs(q4_ctr - q1_ctr)
            
            print(f"      {col}: ìƒê´€ê³„ìˆ˜ {correlation:.4f}, "
                  f"Q1 CTR {q1_ctr:.4f} vs Q4 CTR {q4_ctr:.4f} (ì°¨ì´: {ctr_diff:.4f})")
            
            feature_impacts[col] = {
                'type': 'numeric',
                'correlation': correlation,
                'ctr_diff': ctr_diff
            }
            
        except Exception as e:
            print(f"      {col}: ë¶„ì„ ì‹¤íŒ¨ - {e}")
    
    return feature_impacts

# 4. ë¶ˆê· í˜• ì‹¬í™” ë¶„ì„
def analyze_imbalance_strategies(gdf):
    """í´ë˜ìŠ¤ ë¶ˆê· í˜• ìƒì„¸ ë¶„ì„ ë° ì „ëµ"""
    print(f"\nâš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬í™” ë¶„ì„")
    
    total_samples = len(gdf)
    positive_samples = int(gdf['clicked'].sum())
    negative_samples = total_samples - positive_samples
    
    imbalance_ratio = negative_samples / positive_samples
    positive_pct = (positive_samples / total_samples) * 100
    
    print(f"   ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
    print(f"      ì–‘ì„±(í´ë¦­): {positive_samples:,}ê°œ ({positive_pct:.2f}%)")
    print(f"      ìŒì„±(ë¯¸í´ë¦­): {negative_samples:,}ê°œ ({100-positive_pct:.2f}%)")
    print(f"      ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.1f}:1")
    
    # ë¶ˆê· í˜• ì‹¬ê°ë„ í‰ê°€
    if imbalance_ratio > 100:
        severity = "ê·¹ì‹¬í•¨"
        color = "ğŸš¨"
    elif imbalance_ratio > 20:
        severity = "ì‹¬ê°í•¨"  
        color = "âš ï¸"
    elif imbalance_ratio > 5:
        severity = "ë³´í†µ"
        color = "ğŸ’¡"
    else:
        severity = "ê²½ë¯¸í•¨"
        color = "âœ…"
    
    print(f"   {color} ë¶ˆê· í˜• ì‹¬ê°ë„: {severity}")
    
    # ê¶Œì¥ ì „ëµ
    print(f"\n   ğŸ¯ ê¶Œì¥ ë¶ˆê· í˜• ì²˜ë¦¬ ì „ëµ:")
    
    strategies = []
    
    if imbalance_ratio > 50:
        strategies.extend([
            "Focal Loss (Î³=2, Î±=0.25)",
            "SMOTE + ì–¸ë”ìƒ˜í”Œë§ ì¡°í•©",
            "Cost-sensitive learning (class_weight='balanced')",
            "Ensemble with different sampling"
        ])
    elif imbalance_ratio > 10:
        strategies.extend([
            "Class weight ì¡°ì •",
            "Borderline-SMOTE",
            "Focal Loss (Î³=1)",
            "Threshold moving"
        ])
    else:
        strategies.extend([
            "Class weight ë¯¸ì„¸ ì¡°ì •",
            "Random oversampling",
            "Ensemble diversity"
        ])
    
    for i, strategy in enumerate(strategies, 1):
        print(f"      {i}. {strategy}")
    
    # í‰ê°€ ì§€í‘œ ìµœì í™” ë°©í–¥
    print(f"\n   ğŸ“ˆ í‰ê°€ ì§€í‘œ ìµœì í™”:")
    print(f"      â€¢ Average Precision: ìƒìœ„ ë­í‚¹ í’ˆì§ˆ ì§‘ì¤‘")
    print(f"      â€¢ Weighted LogLoss: í´ë˜ìŠ¤ ê· í˜• 50:50 ìœ ì§€")
    print(f"      â€¢ ìµœì¢… ì ìˆ˜: 0.5 * AP + 0.5 * (1/(1+WLL))")
    
    return {
        'imbalance_ratio': imbalance_ratio,
        'positive_pct': positive_pct,
        'severity': severity,
        'recommended_strategies': strategies
    }

# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def run_deep_analysis():
    """í•µì‹¬ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰"""
    print(f"\n{'='*60}")
    print("ğŸš€ CTR í•µì‹¬ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰")
    print(f"{'='*60}")
    
    # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œ ë¡œë”©
    sample_gdf = load_smart_sample('../data/train.parquet', target_rows=200000)
    
    if sample_gdf is None:
        print("âŒ ìƒ˜í”Œ ë¡œë”© ì‹¤íŒ¨")
        return
    
    print(f"âœ… ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(sample_gdf):,}í–‰ Ã— {len(sample_gdf.columns)}ì»¬ëŸ¼")
    
    # 1. ì‹œê°„ íŒ¨í„´ ë¶„ì„
    temporal_insights = analyze_temporal_patterns(sample_gdf)
    
    # 2. ê²°ì¸¡ê°’ ë¶„ì„
    missing_insights = analyze_missing_patterns(sample_gdf)
    
    # 3. íŠ¹ì„± ì˜í–¥ë„ ë¶„ì„
    feature_impacts = analyze_feature_impact(sample_gdf)
    
    # 4. ë¶ˆê· í˜• ì „ëµ ë¶„ì„
    imbalance_insights = analyze_imbalance_strategies(sample_gdf)
    
    # ìµœì¢… ì¢…í•© ê¶Œì¥ì‚¬í•­
    print(f"\n{'='*60}")
    print("ğŸ¯ ì¢…í•© ê¶Œì¥ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„")
    print(f"{'='*60}")
    
    print(f"\n1ï¸âƒ£ ìš°ì„ ìˆœìœ„ 1: ì‹œê°„ íŠ¹ì„± í™œìš©")
    if 'best_period' in temporal_insights:
        best_period = temporal_insights['best_period']
        print(f"   â€¢ ìµœì  ì‹œê°„ëŒ€: {best_period[0]} (CTR: {best_period[1]:.4f})")
    if 'weekend_boost' in temporal_insights:
        weekend_boost = temporal_insights['weekend_boost']
        print(f"   â€¢ ì£¼ë§ íš¨ê³¼: {weekend_boost:.2f}ë°°")
    print(f"   â€¢ ìˆœí™˜ ì‹œê°„ ì¸ì½”ë”© ì ìš© (sin/cos ë³€í™˜)")
    
    print(f"\n2ï¸âƒ£ ìš°ì„ ìˆœìœ„ 2: ê²°ì¸¡ê°’ ì „ëµ")
    print(f"   â€¢ ì‹¬ê°í•œ ê²°ì¸¡ ì»¬ëŸ¼: {len(missing_insights['severe_missing'])}ê°œ â†’ ì œê±° ê²€í† ")
    print(f"   â€¢ ë³´í†µ ê²°ì¸¡ ì»¬ëŸ¼: {len(missing_insights['moderate_missing'])}ê°œ â†’ ëŒ€ì¹˜ ì „ëµ")
    print(f"   â€¢ ê²°ì¸¡ê°’ ìì²´ë¥¼ íŠ¹ì„±ìœ¼ë¡œ í™œìš© (missing indicator)")
    
    print(f"\n3ï¸âƒ£ ìš°ì„ ìˆœìœ„ 3: ë¶ˆê· í˜• ì²˜ë¦¬")
    severity = imbalance_insights['severity']
    ratio = imbalance_insights['imbalance_ratio']
    print(f"   â€¢ ë¶ˆê· í˜• ì •ë„: {severity} ({ratio:.1f}:1)")
    print(f"   â€¢ 1ì°¨ ì „ëµ: {imbalance_insights['recommended_strategies'][0]}")
    print(f"   â€¢ 2ì°¨ ì „ëµ: {imbalance_insights['recommended_strategies'][1]}")
    
    print(f"\n4ï¸âƒ£ ë‹¤ìŒ ì‹¤í–‰ ë‹¨ê³„:")
    print(f"   1. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶• (XGBoost GPU)")
    print(f"   2. ì‹œê°„ íŠ¹ì„± ê³µí•™ ì ìš©")
    print(f"   3. ë¶ˆê· í˜• ì²˜ë¦¬ A/B í…ŒìŠ¤íŠ¸")
    print(f"   4. CV ì „ëµ ìˆ˜ë¦½ (Time-aware split)")
    print(f"   5. ì•™ìƒë¸” ì „ëµ ì„¤ê³„")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del sample_gdf
    clear_gpu_memory()
    
    print(f"\nâœ… í•µì‹¬ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    run_deep_analysis()