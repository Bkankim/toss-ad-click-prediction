{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20abd96a-4eb5-483d-b3d9-32e1c1340ece",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a593bbe1-b8a5-4a40-b46f-4ff25f132b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e7a28-1d18-40a7-9d19-1dcdab7537ec",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13d5928-880f-4b88-8dfa-dc791e930001",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'BATCH_SIZE': 4096,\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'SEED' : 42\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99aa02da-d89b-4a18-8344-261a413bac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421efbe-70c6-4aeb-946c-be1556e7743d",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0154cbe8-5c69-4d1a-95a6-5f834fc7ab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10704179, 119)\n",
      "Test shape: (1527298, 118)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "all_train = pl.read_parquet(\"../data/train.parquet\")\n",
    "test = pl.read_parquet(\"../data/test.parquet\").drop([\"ID\"])\n",
    "\n",
    "print(\"Train shape:\", all_train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb80e25-5b02-43da-aa4f-5ca16cd7802e",
   "metadata": {},
   "source": [
    "## Data Down-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c0b5e0-7fd7-4c4a-bc80-6a3428204784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicked == 1 데이터\n",
    "clicked_1 = all_train.filter(pl.col(\"clicked\") == 1)\n",
    "\n",
    "# clicked == 0 데이터에서 동일 개수x2 만큼 무작위 추출 (다운 샘플링)\n",
    "clicked_0 = all_train.filter(pl.col(\"clicked\") == 0).sample(\n",
    "    n=clicked_1.height * 2, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 두 데이터프레임 합치기\n",
    "train = (\n",
    "    pl.concat([clicked_1, clicked_0], how=\"vertical\")  # axis=0과 동일\n",
    "      .sample(fraction=1.0, shuffle=True, seed=42)     # frac=1과 동일\n",
    "      # Polars에는 index가 없으므로 reset_index(drop=True)는 불필요\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151fc664-faa0-4951-bbbb-8b92f1ee3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (612537, 119)\n",
      "Train clicked:0: (408358, 119)\n",
      "Train clicked:1: (204179, 119)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Train clicked:0:\", train.filter(pl.col(\"clicked\") == 0).shape)\n",
    "print(\"Train clicked:1:\", train.filter(pl.col(\"clicked\") == 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188a0d4-f60a-422c-bb53-ae8eba8744d6",
   "metadata": {},
   "source": [
    "## Data Column Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ecc473-4bd8-4304-9d68-a5c01f4bbe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 117\n",
      "Sequence: seq\n",
      "Target: clicked\n"
     ]
    }
   ],
   "source": [
    "# Target / Sequence\n",
    "target_col = \"clicked\"\n",
    "seq_col = \"seq\"\n",
    "\n",
    "# 학습에 사용할 피처: ID/seq/target 제외, 나머지 전부\n",
    "FEATURE_EXCLUDE = {target_col, seq_col, \"ID\"}\n",
    "feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]\n",
    "\n",
    "print(\"Num features:\", len(feature_cols))\n",
    "print(\"Sequence:\", seq_col)\n",
    "print(\"Target:\", target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426899fa-b4f3-480d-b7de-8d7e811ad4f8",
   "metadata": {},
   "source": [
    "## Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c0b13b-3a81-4a2e-a753-1f9d8aa467ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True):\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "\n",
    "        # 비-시퀀스 피처: 전부 연속값으로\n",
    "        self.X = self.df.select(self.feature_cols).cast(pl.Float64).fill_null(0).to_numpy()\n",
    "\n",
    "        # 시퀀스: 문자열 그대로 보관 (lazy 파싱)\n",
    "        self.seq_strings = self.df[self.seq_col].cast(str).to_numpy()\n",
    "\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].cast(pl.Float32).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.height\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float)\n",
    "\n",
    "        # 전체 시퀀스 사용 (빈 시퀀스만 방어)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array([], dtype=np.float32)\n",
    "\n",
    "        if arr.size == 0:\n",
    "            arr = np.array([0.0], dtype=np.float32)  # 빈 시퀀스 방어\n",
    "\n",
    "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
    "\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return x, seq, y\n",
    "        else:\n",
    "            return x, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b95f1cd-e8e8-4420-ad26-c639e36e472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch):\n",
    "    xs, seqs, ys = zip(*batch)\n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)  # 빈 시퀀스 방지\n",
    "    return xs, seqs_padded, seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    xs, seqs = zip(*batch)\n",
    "    xs = torch.stack(xs)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return xs, seqs_padded, seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649f6ff-cb10-45ee-b787-d2e7e652c871",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9144bc61-0942-4ed3-bb52-e347d571ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularSeqModel(nn.Module):\n",
    "    def __init__(self, d_features, lstm_hidden=32, hidden_units=[1024, 512, 256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        # 모든 비-시퀀스 피처에 BN\n",
    "        self.bn_x = nn.BatchNorm1d(d_features)\n",
    "        # seq: 숫자 시퀀스 → LSTM\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden, batch_first=True)\n",
    "\n",
    "        # 최종 MLP\n",
    "        input_dim = d_features + lstm_hidden\n",
    "        layers = []\n",
    "        for h in hidden_units:\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        layers += [nn.Linear(input_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_feats, x_seq, seq_lengths):\n",
    "        # 비-시퀀스 피처\n",
    "        x = self.bn_x(x_feats)\n",
    "\n",
    "        # 시퀀스 → LSTM (pack)\n",
    "        x_seq = x_seq.unsqueeze(-1)  # (B, L, 1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = h_n[-1]                  # (B, lstm_hidden)\n",
    "\n",
    "        z = torch.cat([x, h], dim=1)\n",
    "        return self.mlp(z).squeeze(1)  # logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ad06-d840-4ac3-b565-4d2450c0af39",
   "metadata": {},
   "source": [
    "## Train / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d94d11-f7b9-449e-b631-e70224df92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, feature_cols, seq_col, target_col,\n",
    "                batch_size=512, epochs=3, lr=1e-3, device=\"cuda\"):\n",
    "\n",
    "    # 1) split\n",
    "    tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    # 2) Dataset / Loader (l_max 인자 제거)\n",
    "    train_dataset = ClickDataset(tr_df, feature_cols, seq_col, target_col, has_target=True)\n",
    "    val_dataset   = ClickDataset(va_df, feature_cols, seq_col, target_col, has_target=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn_train)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
    "\n",
    "    # 3) 모델\n",
    "    d_features = len(feature_cols)\n",
    "    model = TabularSeqModel(d_features=d_features, lstm_hidden=64, hidden_units=[256,128], dropout=0.2).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4) Loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xs, seqs, seq_lens, ys in tqdm(train_loader, desc=f\"Train Epoch {epoch}\"):\n",
    "            xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xs, seqs, seq_lens)\n",
    "            loss = criterion(logits, ys)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * ys.size(0)\n",
    "        train_loss /= len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xs, seqs, seq_lens, ys in tqdm(val_loader, desc=f\"Val Epoch {epoch}\"):\n",
    "                xs, seqs, seq_lens, ys = xs.to(device), seqs.to(device), seq_lens.to(device), ys.to(device)\n",
    "                logits = model(xs, seqs, seq_lens)\n",
    "                loss = criterion(logits, ys)\n",
    "                val_loss += loss.item() * len(ys)\n",
    "        val_loss /= len(val_dataset)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0999f03-85ca-4eaf-bea0-822cda393d29",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b730bdf4-9697-40b1-b53b-1c10380ff43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 120/120 [02:36<00:00,  1.30s/it]\n",
      "Val Epoch 1: 100%|██████████| 30/30 [00:27<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.5909 | Val Loss: 0.5729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 120/120 [02:35<00:00,  1.30s/it]\n",
      "Val Epoch 2: 100%|██████████| 30/30 [00:26<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.5739 | Val Loss: 0.5685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 120/120 [02:36<00:00,  1.30s/it]\n",
      "Val Epoch 3: 100%|██████████| 30/30 [00:26<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.5695 | Val Loss: 0.5655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 120/120 [02:38<00:00,  1.32s/it]\n",
      "Val Epoch 4: 100%|██████████| 30/30 [00:27<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.5679 | Val Loss: 0.5652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 120/120 [02:38<00:00,  1.32s/it]\n",
      "Val Epoch 5: 100%|██████████| 30/30 [00:27<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.5662 | Val Loss: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 120/120 [02:37<00:00,  1.32s/it]\n",
      "Val Epoch 6: 100%|██████████| 30/30 [00:27<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.5650 | Val Loss: 0.5630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 120/120 [02:39<00:00,  1.33s/it]\n",
      "Val Epoch 7: 100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.5636 | Val Loss: 0.5630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 120/120 [02:40<00:00,  1.34s/it]\n",
      "Val Epoch 8: 100%|██████████| 30/30 [00:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.5625 | Val Loss: 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 120/120 [02:41<00:00,  1.35s/it]\n",
      "Val Epoch 9: 100%|██████████| 30/30 [00:26<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.5615 | Val Loss: 0.5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 120/120 [02:39<00:00,  1.33s/it]\n",
      "Val Epoch 10: 100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.5606 | Val Loss: 0.5604\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\n",
    "    train_df=train,\n",
    "    feature_cols=feature_cols,\n",
    "    seq_col=seq_col,\n",
    "    target_col=target_col,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    epochs=CFG['EPOCHS'],\n",
    "    lr=CFG['LEARNING_RATE'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5421c1-a18d-43de-934e-d0ee5af34594",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f64e9288-e35d-4e03-bb41-2c4d1fd5ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 373/373 [05:22<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1) Dataset/Loader\n",
    "test_ds = ClickDataset(test, feature_cols, seq_col, has_target=False)\n",
    "test_ld = DataLoader(test_ds, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_infer)\n",
    "\n",
    "# 2) Predict\n",
    "model.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for xs, seqs, lens in tqdm(test_ld, desc=\"Inference\"):\n",
    "        xs, seqs, lens = xs.to(device), seqs.to(device), lens.to(device)\n",
    "        outs.append(torch.sigmoid(model(xs, seqs, lens)).cpu())\n",
    "\n",
    "test_preds = torch.cat(outs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4935d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TabularSeqModel' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 실행\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mplot_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mplot_feature_importance\u001b[0;34m(model, feature_cols, top_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_feature_importance\u001b[39m(model, feature_cols, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      2\u001b[0m     fi_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: feature_cols,\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n\u001b[1;32m      5\u001b[0m     })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(top_n)\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      8\u001b[0m     sns\u001b[38;5;241m.\u001b[39mbarplot(data\u001b[38;5;241m=\u001b[39mfi_df, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TabularSeqModel' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "def plot_feature_importance(model, feature_cols, top_n=20):\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": feature_cols,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(\"importance\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=fi_df, x=\"importance\", y=\"feature\", palette=\"viridis\")\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 실행\n",
    "plot_feature_importance(model, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc0a92-05f9-4be6-9e45-cc52240afc3e",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "757f01bc-51a2-46bf-983f-a54e91abe222",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pl.read_csv(\"../data/sample_submission.csv\")\n",
    "submit = submit.with_columns(pl.Series(\"clicked\", test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "289fc0a5-d97a-4cd6-abfb-245b7383ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.write_csv(\"./baseline_submit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
